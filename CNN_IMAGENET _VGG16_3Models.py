# -*- coding: utf-8 -*-
"""David_Gargaro_845738_assignment3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpp_eXm-yMRQNzxIe1LVaoH1sguUz5t_
"""

# import dependencies
import tensorflow as tf
from tensorflow import keras
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.preprocessing import image
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import metrics
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
keras.utils.set_random_seed(42)

"""### Preprocessing del dataset

Il dataset utilizzato per il task di classificazione è CIFAR10.

Esso presenta 10 classi differenti:

| Label       | Description    |
| ----------- | ---------------|
| 0           | airplane       |
| 1           | automobile     |
| 2           | bird           |
| 3           | cat            |
| 4           | deer           |
| 5           | dog            |
| 6           | frog           |
| 7           | horse          |
| 8           | ship           |
| 9           | truck          |
"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""Immagine del dataset come esempio"""

plt.imshow(x_train[18])

"""Causa limitazioni di colab è stato necessario ridurre il dataset originale"""

TRAIN_SIZE = 500
TEST_SIZE = 120

"""La funzione sottostante permetterà la creazione di un nuovo dataset. Esso sarà un sottoinsieme dell'originale."""

def split_dataset(x, y, size):
  l = [0] * 10
  y = np.reshape(y, (y.shape[0]))
  new_x = np.zeros((size, x.shape[1], x.shape[2], x.shape[3]), dtype=np.float32)
  new_y = np.zeros((size, 1), dtype=np.uint8)
  j = 0
  for i in range(y.shape[0]):
    if l.count(size//10) == len(l):
      del y
      return new_x, new_y
    else:
      if l[y[i]] < size//10:
        l[y[i]] += 1
        new_y[j] = [y[i]]
        new_x[j] = x[i]
        j += 1

dataset_train = split_dataset(x_train, y_train, TRAIN_SIZE)
dataset_test = split_dataset(x_test, y_test, TEST_SIZE)

print(dataset_train[0].shape)
print(dataset_train[1].shape)
print(dataset_test[0].shape)
print(dataset_test[1].shape)

"""Numero di elementi per classe"""

np.unique(dataset_train[1], return_counts = True)

"""Mostro un'immagine del nuovo dataset con la sua label.

In questo caso la label risulta essere 2 ovvero bird.
"""

plt.imshow(dataset_train[0][18].astype(np.uint8))
print(dataset_train[1][18])

"""Ora viene effettuato un resize di tutte le immagine portandole da 32x32 a 224x224.

Questa operazione è necessaria per via dell'input di VGG16.
"""

x_train_scaled = tf.image.resize(dataset_train[0], (224, 224))
x_test_scaled = tf.image.resize(dataset_test[0], (224, 224))

y_train = dataset_train[1]
y_test = dataset_test[1]

# Scale images into the [0 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

# Make images with size (224,224,1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

x_train_scaled = preprocess_input(x_train_scaled)
x_test_scaled = preprocess_input(x_test_scaled)

print(x_train_scaled.shape)
print(x_test_scaled.shape)
print(y_train.shape)
print(y_test.shape)

"""L'immagine dopo il processo risulta essere:"""

display(tf.keras.utils.array_to_img(x_train_scaled[18]))

"""### Models

Il modello su cui fare features extraction è VGG16 con i pesi di imagenet.
"""

# load vgg model
vgg_model = VGG16(weights='imagenet')

vgg_model.summary()

"""Model1

Taglio effettuato quasi all'inizio, al primo livello di maxpool.
"""

model_first = tf.keras.Model(inputs=vgg_model.input, outputs=vgg_model.get_layer("block2_pool").output)
model_first.summary()

"""Now that we have defined the model for feature extraction, let's use it to extract the features."""

features_first = model_first.predict(x_train_scaled)
print(features_first.shape)

features_test_first= model_first.predict(x_test_scaled)
print(features_test_first.shape)

"""Since we want to train a SVM classifier, we have to convert the multi-dimensional array into a vector. For this, we just flatten the last dimensions."""

features_reshaped_first = features_first.reshape(features_first.shape[0], -1)
test_features_reshaped_first = features_test_first.reshape(features_test_first.shape[0], -1)

# check result
print(features_reshaped_first.shape)
print(test_features_reshaped_first.shape)

svm_classifier1 = SVC(kernel='linear', C=1)
svm_classifier1.fit(features_reshaped_first, y_train.flatten())

predictions1 = svm_classifier1.predict(test_features_reshaped_first)
print(classification_report(y_test, predictions1))

"""Metrics"""

acc = []
acc.append(metrics.accuracy_score(y_test, predictions1))
print("Accuracy:", metrics.accuracy_score(y_test, predictions1))
print("Precision:", metrics.precision_score(y_test, predictions1, average='macro'))
print("Recall:", metrics.recall_score(y_test, predictions1, average='macro'))

"""Model 2

Taglio effettuato quasi a metà, al terzo livello di maxpool.
"""

model_second = tf.keras.Model(inputs=vgg_model.input, outputs=vgg_model.get_layer("block3_pool").output)
model_second.summary()

features_second = model_second.predict(x_train_scaled)
print(features_second.shape)

features_test_second= model_second.predict(x_test_scaled)
print(features_test_second.shape)

features_reshaped_second = features_second.reshape(features_second.shape[0], -1)
test_features_reshaped_second = features_test_second.reshape(features_test_second.shape[0], -1)

# check result
print(features_reshaped_second.shape)
print(test_features_reshaped_second.shape)

svm_classifier2 = SVC(kernel='linear', C=1)
svm_classifier2.fit(features_reshaped_second, y_train.flatten())

predictions2 = svm_classifier2.predict(test_features_reshaped_second)
print(classification_report(y_test, predictions2))

acc.append(metrics.accuracy_score(y_test, predictions2))
print("Accuracy:", metrics.accuracy_score(y_test, predictions2))
print("Precision:", metrics.precision_score(y_test, predictions2, average='macro'))
print("Recall:", metrics.recall_score(y_test, predictions2, average='macro'))

"""Model 3

Taglio effettuato a fine modello, precisamente al quinto livello di maxpool.
"""

model_third = tf.keras.Model(inputs=vgg_model.input, outputs=vgg_model.get_layer("block5_pool").output)
model_third.summary()

features_third = model_third.predict(x_train_scaled)
print(features_third.shape)

features_test_third= model_third.predict(x_test_scaled)
print(features_test_third.shape)

features_reshaped_third = features_third.reshape(features_third.shape[0], -1)
test_features_reshaped_third = features_test_third.reshape(features_test_third.shape[0], -1)

# check result
print(features_reshaped_third.shape)
print(test_features_reshaped_third.shape)

svm_classifier3 = SVC(kernel='linear', C=1)
svm_classifier3.fit(features_reshaped_third, y_train.flatten())

predictions3 = svm_classifier3.predict(test_features_reshaped_third)
print(classification_report(y_test, predictions3))

acc.append(metrics.accuracy_score(y_test, predictions3))
print("Accuracy:", metrics.accuracy_score(y_test, predictions3))
print("Precision:", metrics.precision_score(y_test, predictions3, average='macro'))
print("Recall:", metrics.recall_score(y_test, predictions3, average='macro'))

"""### Conclusioni

Confusion Matrix del primo modello
"""

cm = confusion_matrix(y_test, predictions1)
fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(10,10))
plt.show()

"""Confusion Matrix del secondo modello"""

cm = confusion_matrix(y_test, predictions2)
fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(10,10))
plt.show()

"""Confusion Matrix del terzo modello"""

cm = confusion_matrix(y_test, predictions3)
fig, ax = plot_confusion_matrix(conf_mat=cm, figsize=(10,10))
plt.show()

color = ['orange', 'royalblue', 'crimson']
x = ['model1', 'model2', 'model3']
f, ax = plt.subplots(figsize=(13, 6))
ind = np.arange(len(acc))
ax.set_title('Accuracy tra i modelli')
plt.barh(x, acc, color=color)
plt.xlabel('Accuracy')
plt.ylabel('Modelli')
for i, v in enumerate(acc):
  ax.text(v + 0.01, i, str(f'{v:.2f}'), color='black')
plt.show()

"""Il dataset è stato diviso in 500 istante per il training set e 120 per il test set a causa di problemi di memoria in colab.

E' stata utilizzata una CNN preaddestrata su IMAGENET con architettura VGG16.
Sono stati creati 3 modelli differenti basati sulla stessa CNN ma con tagli differenti. I tagli vengono effettuati all'inizio, a metà ed alla fine del CNN.
- il primo modello tagliato su block2_pool
- il secondo modello tagliato su block3_pool
- l terzo modello tagliato su block5_pool

Come si può notare, più si taglia in profondità più l'accuracy cresce.

in particolare:
- il primo modello ha accuracy = 0.42
- il secondo modello ha accuracy = 0.50
- il terzo modello ha accuracy = 0.66
"""